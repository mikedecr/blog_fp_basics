---
author: Michael DeCrescenzo
categories: [code, r]
title: |
    More than `lapply`: functional programming adventures in R
# subtitle: | "That's it. Every blog post gets its own repository" and other adventures
# excerpt: |
#     A blogdown website is more like a combination of modular components that are better managed independently, each component with its own repository.
#     These modular components _come together_ at the nexus of the website, but the components do not _belong to_ the website.
date: "2022-08-17"
knitr:
    opts_chunk:
        collapse: true
draft: false
# image: "git_featured.png"
---

When people talk about "functional programming" in R, they usually mean two things.

1. Many intermediate R users have a _thing_ against for-loops because for-loops have a reputation for being slow in R,[^loops]
   instead advocating for `apply` functions or `purrr::map` in a nested-data-frame context.
   (I will explain `apply` and `map` below.)
   This is popularly referred to in these circles as "functional programming" because,
   rather than procedurally manipulating data explicitly,
   the approach takes a pre-defined function and merely applies it to data.
2. More advanced R discussions (see the book literally titled [Advanced R](https://adv-r.hadley.nz/fp.html))
   refer to the fact that functions are first-class objects in R, 
   meaning that you can look at their properties or do operations on them,
   just like you can do to other data structures.

[^loops]: This reputation may be unfair.
        Most of the time, loops are slow because people are using them to do unwise things,
        like growing the size of an array in each iteration.
        This causes R to make a lot of copies of data that would be avoided if the user were more careful.
        Using `apply` functions solves this problem for them either way.

By the numbers, most R users are more probably familiar with the meaning of [1] than [2],
which is a shame because functional paradigms outside of R go way deeper, and get _way_ weirder, than `apply(function, data)`.
This post will be an attempt to explain a little of this weirdness and implement it in R.

Readers who are experienced with functional programming will recognize that this post doesn't get _so_ detailed.
They may also realize that underneath the tidyverse interface are a _lot_ of functional ideas, notably tidy evaluation.
This post keeps the material pretty light, and we won't get into tidy evaluation.
Maybe some other time.

## What is functional programming?

Let's do the Wikipedia thing.
From the [functional programming](https://en.wikipedia.org/wiki/Functional_programming) article:

> In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. 

You may be thinking, "That sounds pretty unremarkable.
I already know that I write functions and apply them.
Why do we need a name for this?"

Well... read on:

> It is a declarative programming paradigm in which function definitions are trees of expressions that map values to other values,
> rather than a sequence of imperative statements which update the running state of the program.

There are a lot of information-dense parts of that sentence that may be difficult to appreciate without knowing more of the theory.
But let's try to break it apart.
Starting in the middle.

### "Functions map values to other values"

You have probably seen function definition written like this,
$$
y = a + bx
$$
and we say that $y$ is a function of $x$.
We can label the transformation of $x$ as $f$ and say $y = f(x)$.
Easy!

But there is another way to write statements like this that you may have seen in papers that have high opinions of their own notation:
$$
f : X \mapsto Y
$$ {#eq-map}
which reads, "$f$ is a _map_ from $X$ to $Y$", where capital-$X$ and capital-$Y$ are _sets_ of values from which the single values $x$ and $y$ are drawn.
Which is to say: $f$ is simply a thing that eats a value from $X$ and spits out a value from $Y$.
In this example, the value of $y$ that it returns is $a + (b \times x)$.[^category]

[^category]: You may have assumed that $x$ and $y$ are real (that is, $\mathbb{R}$) numbers and that the operations $+$ and $\times$ refer to the sum and product operations that we conventionally apply to real numbers.
           It's fine if you made those assumptions, but it wasn't necessary.
           One cool thing about this family of math is that you can let $X$ and $Y$ be other kinds of sets containing god-knows-what, and as long as $+$ and $\times$ refer to operations on values from those sets that maintain certain properties, the $f$ map remains perfectly coherent.
           [Category theory](https://www.youtube.com/watch?v=I8LbkfSSR58&list=PLbgaMIhjbmEnaH_LTkxLI7FMa2HsnawM_&index=1) tells us about which properties must be maintained for this expression to hold generally.

### "Function definitions are _trees_ of expressions ..." 

Referring to functions as "trees" won't make sense until we talk about **function composition** and **associativity**.

First, composition.
If the notation in [@eq-map] is new to you, your mental picture of function composition probably looks like this.
We can take two functions $f$ and $g$...
\begin{align}
    y &= f(x) \\
    z &= g(y) 
\end{align}
and put them together...
$$ \begin{align} z &= g(f(x)) \end{align} $$ {#eq-nested}
and then we can represent that composition with a different symbol $h$, so that that $h(x)$ is equivalent to $g(f(x))$.

There's nothing incorrect about that approach, but it is verbose.
We actually don't have to refer to any function arguments or values---$x$, $y$, or $z$---to introduce $h$.
Instead we can define $h$ "point-free":
$$ h = g \circ f $$ {#eq-composition}
where the symbol $\circ$ refers to function composition (in a leftward direction).
The expression $g \circ f$ is a new function, which we can speak aloud as "$g$ compose $f$" or "$g$ after $f$".
Because the composition is itself a function, we can pass it an argument: $(g \circ f)(x)$ would spit out the value $z$, just like in [@eq-nested].

Okay, next point.
Associativity.
Function composition is associative, which means compositions can be "grouped" in whatever way you want as long as you don't rearrange the ordering of the base functions.

To demonstrate, let's write some functions out in "map" notation, and for the sake of simplicity let's say that each is a map from a real number to a real number.
\begin{align}
    \newcommand{\R}{\mathbb{R}}
    f &: \R \mapsto \R \\
    g &: \R \mapsto \R \\
    j &: \R \mapsto \R 
\end{align}
Now we want to compose these functions.
Because composition is associative, we could write the composition many ways:

- As one single chained composition: $j \circ g \circ f$, which if applied to $x$ would be equivalent to $j(g(f(x)))$.
- Introduce $h = g \circ f$ as above, and rewrite as $j \circ h$. This is the same as $j \circ (g \circ f)$ or $j(h(x))$.
- Compose $j$ and $g$ into some $d$ and say $d \circ f$, which is the same as $(j \circ g) \circ f$ or $d(f(x))$.

All of these expressions are equivalent.

Okay, recap.
As long as the output of one map is _the same "type"_ as the input to the some other map, we can compose those two functions.
And we can take a series of compositions and group them in whatever way we want, as long as the input and output types conform.

**But how is this a "tree"?**
I will make a little graph with nodes and edges that shows the basic idea.
Caveats up front that this will be pretty informal, but let's say the functions are nodes, and the edges point in the direction of composition.
Here is an example showing $a = j \circ g \circ f$ as a tree.

```{dot}
#| fig-align: center
#| label: fig-tree
digraph F {
    layout=dot
    j -> a;
    g -> a;
    f -> a;
}
```

The tree lets us express associativity in a different way: we can pre-compose any two functions by creating another node from them.

```{dot}
#| fig-align: center
digraph F {
    layout=dot
    j -> a;
    h -> a;
    g -> h;
    f -> h;
}
```

We can see that the composition is associative because the composition of $f$ and $g$ into $h$ doesn't change the order in which functions are applied to form $a$.
We can "pre-group" functions in the tree however we want, but we can't change their ordering except in special circumstances.
Composition is associative, but not all functions are commutative.


### "...Rather than a sequence of imperative statements that update the running state."

Most of the time when we do data analysis, we write code that updates the _state_ of some data.
Say I start with some $x$, and then I do functions $f$, $g$, and $j$ to it.
```{r}
#| eval: false
y = f(x)
z = g(y)
w = j(z)
```
There is some data `x` that we alter, in steps, by doing things to it, each time saving some data at some intermediate state.

The functional approach would be different.
Instead of doing most of our typing passing data around, we create functions.
```{r}
#| eval: false
h = compose(g, f)
a = compose(j, h)
```
And only later (say, in some `main()` routine) do we apply our ultimately-composed functions to data.
The data have to get acted on eventually, but we do it only after we compose a map from the data to some endpoint.

Stated a different way, when we do more imperative programming, the present state of `x` is always our concern.
Our path from beginning to end requires us to leave `x` in some intermediate state at many different steps, and the statefulness of `x` causes a lot of problems for us as we try to plot our way to our destination.
When we do functional programming, however, we simply write the recipe for what we will do to `x` before we touch `x` at all.
We prefer not to let the intermediate state become visible.
The intermediate state is, after all, not that useful.

This might be hard to envision because we haven't invented functional programming yet, so let's invent it.

## Functional fundamentals, with R

We saw a notational approach to function composition above that looked like this:
\begin{align}
w = (j \circ g \circ u)
\end{align}
If we could do that in a computer, it might look like this:

```{r}
#| eval: false
w = compose(j, g, f)
```

There are two important things to remember about this.

1. Function composition creates a _new function_.
   It does not immediately create data.
   In the example, `w` is the composition, which is only evaluated on data when it is necessary.
   It may not be evaluated on data at all directly---we will likely compose `w` with another function before we even think about data.
   If you have heard of _delayed evaluation_, this is probably why.
   You were doing something that created new functionality instead of new data.
2. Function compositions are themselves composable, so we want a framework to compose a lot of functions all at once with whatever associative groupings we want.
   We saw above (with math) that we could compose an arbitrary sequence of functions and the result should be well-behaved (as long as their input and output types conform---more on that in a bit).
   We would like to achieve that behavior in the code as well.

We start with a primitive operation to compose just two functions.
We call it `compose_once`---it implements only one composition of a left-hand and a right-hand function.

```{r}
# returns the new function: f . g
compose_once = function(f, g) {
    function(...) f(g(...))
}
```

Read this closely.
`compose_once` takes two arguments, and those arguments are _functions_.
We do not know or care what those functions are.
We also return a new function, rather than some value.
That new function defines a recipe for evaluating the functions $f$ and $g$ in sequence on some unknown inputs `...`, whever it happens to come across those inputs---we didn't pass any `...` yet.
This is also on purpose.
Composition does not care what the data are.
It only knows how to evaluate functions in order.

This might feel weird at first, but it will give us legible behavior right from the start.
For example, we often want to know how many unique elements are in some data object.
In base R, we might ask `length(unique(x))`.
The `dplyr` package provides a combined function `n_unique(x)`, but we haven't invented `dplyr` yet, so we have to make `n_unique` ourselves:

```{r}
# we save the function returned by compose_once
# no computation on data is done yet.
n_unique = compose_once(length, unique)

# examine the object to convince yourself of this
n_unique

# apply the function on some data
x = c(0, 0, 0, 1, 2)
n_unique(x)
```

`compose_once` lets us compose two functions, but we already know that function composition lets us compose however many functions we want (as long as the output type from one function conforms with the input type of next function).
So we extend this to an arbitrary series of functions by performing a _reduce_ across an array of functions.

```{r}
# pass vector fns = c(f1, f2, ..., fn)
compose = function(fns) {
    Reduce(compose_once, fns, init=identity)
}
```

If you aren't familiar with reductions, they are an efficient trick to collapse an array (loosely speaking) of input values into one output value, accumulating at each step the results of some binary operation.
If you want to learn more about reduction, follow this footnote.[^reduce]
We also supply an argument to `init`, which isn't strictly necessary in this R example but is interesting (IMO) in the mathematical context of function composition, which I explain in this other footnote.[^init]

[^reduce]: You may have seen a reduction in an operation like `max()`---for an array of numbers, recursively check to see if `left` is greater than `right`, passing the winner to the next iteration as `left`.
         In this case, however, the array we are collapsing is full of functions rather than, say, numbers.
         And the operation we apply as we accumulate is `compose_once(left, right)` rather than `left > right`.
         The result of the reduction is a function that encodes the composition of all functions in `fns` from left to right.

[^init]: The role of an initialization in a reduction is to handle an identity condition: how do we handle "empty" values along the reduction without affecting the output.
        Stated differently, what value can we pass to ensure a "no-op" when the reduction is applied, which can be used as an initial value to `left`.
       For example, if we reduce an array of numbers by _addition_, the initialization would be the value `0`, because applying `+ 0` to any number gives you the original number.
       Same with multiplication and the value `1`.
       When we are reducing _functions_ by _composing_ them, we use the identity function.
       Not surprisingly, the identity function simply returns the function arguments unaffected: `identity(x)` gives you `x`.
       For function composition, `compose(f, identity)` is equal to `compose(identity, f)` is equal to `f`.
       Now you're programmin with powerful abstractions!

Let's see it in action.
Let's do an additional step to convert `n_unique` into an English word.

```{r}
# same as n_unique but convert to a word
english_n_unique = compose(c(english::english, length, unique))

# apply to our vector x
english_n_unique(x)
```

And just to underscore your confidence in the associativity of composition, we can exhaustively group these compositions without affecting the results.

```{r}
compose(c(english::english, compose(c(length, unique))))(x)
compose(c(compose(c(english::english, length)), unique))(x)
```


### Types are our guide.

You may have seen online discussions about "strong typing".
Types are representations of data in the computer.
We have basic data types like short integers, long integers, characters, and strings.
Languages implement other data structures like lists, dictionaries, arrays, tuples, etc. that we can also call types for our purposes.

You may have seen some people discuss the "type flexibility" of R or Python as advantages.
Well...sorry!
Functional programming like strong types because strong types provide structure to function composition at scale.
Functions can be composed if the output type of one function is the input type of the next function.
If we can trust this fact, we can build really big, abstract creations out of function composition, maybe even creations that are so big and complicated that we struggle to keep track of it all in our brains.
But these creations are _virtually guaranteed to work_ as long as they are composed of functions with conforming input and output types.

Not coincidentally, the associativity of function composition is additionally helpful for API design.
If we have to build a big, abstract structure, we always have the option to group some of the operations together if they perform a coherent and useful operation.
Intermediate functions are more useful than intermediate state because at least an intermediate function is legible and potentially reusable. 
Chances are your intermediate data are not that legible or useful.

Let's look at the "type roadmap" for the `english_n_unique` example.
We started with a vector type and mapped to an `english` type.
How did we know it would work?
We can diagram the types.
$$
\begin{align}
   \mathtt{unique} &: \mathtt{many} \mapsto \mathtt{vector} \\
   \mathtt{length} &: \mathtt{many} \mapsto \mathtt{integer} \\
   \mathtt{english::english} &: \mathtt{many} \mapsto \mathtt{vector}
\end{align}
$$
We know that `unique` takes objects of various types (which I represent as `many` to say "this function has many methods for different data types") and returns a vector of unique elements.
We know that length takes `many` and returns its integer length, and that should work for a vector input.
And `engligh::english` turns `many` objects into `english` type, which ought to work for an integer input.
But we know that these operations should compose because we can know that we can pass a vector to `unique`, a vector to `length`, and an integer to `english::english`.
Stated differently, if I know the type behavior of each function, I know the type behavior of the compositions.
That is an extremely useful and powerful foundation for building big things.


## Something bigger: means within groups

This is where we really start to see the difference between typical "imperative" programming and functional programming "trees".
Let's take something we commonly do in R, calculate means within groups of data, and do it with our functional tools.
Let's take the `mtcars` data frame...
```{r}
head(mtcars, 5)
```
...and calculate the mean of each variable as a new data frame with one row per group.
We want to be able to specify the group on the fly.

We want to chart a course through this problem that composes small, re-usable steps on well-defined types.
Here is my plan:

1. define a function that maps a data frame of raw data to a data frame of column means.
2. define a function to _any_ function to groups of data
3. compose a function that implements a split-apply-combine which is the composition of the above steps.

### 1. Means for a data frame.

We create a function by composing the following functions that map the following types:

- `colMeans`: data frame $\mapsto$ vector
- `as.list`: vector $\mapsto$ list
- `tibble::as_tibble`: list $\mapsto$ data_frame

You can see how these functions take us incrementally from data frame, to vector, to list, back to data frame.
So we these operations should compose nicely.

```{r}
means = compose(c(tibble::as_tibble, as.list, colMeans))
```

Testing it on the full data:
```{r}
means(mtcars)
```

### 2. Apply a function over groups

Remember, functional programming hasn't been invented yet, so we can't simply do his with `lapply`.
We can, however, reimplement `lapply` knowing what we know about functional programming.

Let's create an function that takes any function `f` and an iterable object `l`, and returns a list.

```{r}
apply_on_elements = function(l, f) {
    # initialize an empty list
    v = vector(mode = "list", length = length(l))
    # assign f(x) for each x in l
    for (x in seq_along(l)) {
        v[[x]] = f(l[[x]])
    }
    return(v)
}
```

Notice, this function takes _two_ objects as arguments and returns one object.
In order to nicely compose it with other functions that take and return only _one_ object, I want a way to reduce the arguments required when I call it.
This is a little weird, but I am going to create something like a Python tool called a `partial`, which takes a function `f` and returns a new function `f*`, which is just like `f` but with some of the arguments pre-filled.
Let's define it before I explain further:

```{r}
partial_apply = function(f) {
    function(x) apply_on_elements(x, f)
}
```

So `partial_apply` takes a function `f` and returns a _new_ function.
That new function applies `f` to some iterable object `x` to return a list.
But the function isn't evaluated; it is only created, because I never provide an `x` in the outer function.
The user has to pass `x` to evaluate the partial at a later time.

Here we use this tool to create a partial `length`, which we apply to some `x`.

```{r}
lengths = partial_apply(length)

# examine, it's a function
lengths

lengths(x)
```

We see more of that delayed evaluation behavior.
This is helpful to let us create a function that applies our earlier `means` to groups of data.

```{r}
partial_apply(means)
```


### 3. Apply our partial function to data.

Rather, create a function that _would_ do that, if it were evaluated.

We want to split a data frame into a list, apply `means` to each element of that list, and return a final data frame.
Here's how we map these steps from type to type.

- `split`: pair of (data frame, vector) $\mapsto$ list
- `partial_apply(means)`: list $\mapsto$ list
- `dplyr::bind_rows`: list $\mapsto$ data frame

The composition of all these steps creates a function that eats a data frame and returns a data frame.

```{r}
means_by = compose(c(dplyr::bind_rows, partial_apply(group_means), split))
```

### Put it all together.

The code below retraces our steps.
Step (1) creates our `means` function.
Step (2) creates some functional infrastructure to apply functions over iterable objects, which isn't the kind of thing we would ordinarily have to mess with as end-users.
And step (3) composes our `means` function with the iteration tools to make our eventual result.

```{r}
# 1. small function to be applied
means = compose(c(tibble::as_tibble, as.list, colMeans))

# 2. infrastructure layer:
# you can see why these would be generally useful for many problems
# 2a. reimplement *-apply()
apply_on_elements = function(l, f) {
    v = vector(mode = "list", length = length(l))
    for (x in seq_along(l)) {
        v[[x]] = f(l[[x]])
    }
    return(v)
}
# 2b. partial apply
partial_apply = function(f) {
    function(x) apply_on_elements(x, f)
}


# 3. interface layer
means_by = compose(c(dplyr::bind_rows, partial_apply(means), split))
```

Again, given that step (2) is rebuilding the wheel, it's pretty impressive how little code goes into steps (1) and (3) to achieve the end results.
The final API in this iteration certainly isn't as succinct as `dplyr` grouping and summarizing, but remember, functional programming hasn't been invented yet.
But moreover, you can start to imagine how tools like partials can be stacked to create tools that are essentially as powerful as `dplyr`, even if their interfaces are different (more functional).


Let's apply the ultimate function, `means_by`, to various groups in `mtcars`.

```{r}
means_by(mtcars, mtcars$cyl)
means_by(mtcars, mtcars$vs)
means_by(mtcars, mtcars$am)
means_by(mtcars, mtcars$gear)
means_by(mtcars, mtcars$carb)
```


### Notes on the tidyverse

Now that we have invented functional programming, we can better appreciate how tidyverse tools leverage functional infrastructure to make nice APIs, even if those APIs feel way less hardcore-functional than the example we just created.

**The pipe operator**.
You may have thought to yourself, composing "left" sure is harder to read than composing "right" like the pipe operator.
First, functional programming hadn't been invented yet, so you can't blame me for not knowing about the pipe operator.
Second, creating a composition function that reads more "linearly" is easy with one extra step: reversing the direction of the `Reduce`.

```{r}
pipe = function(fns) {
    # reverse the order of fns before composing
    compose(rev(fns))
}
```

Now we can write our `means_by` function with more linear recipe that reminds us more of tidyverse code.

```{r}
means_by = pipe(c(split, partial_apply(means), dplyr::bind_rows))
means_by(mtcars, mtcars$cyl)
```

**Partial functions**.
The `partial_apply` function might have been the weirdest-feeling step in the earlier example.
But if you squint at it, you realize that this is sort of what the tidyverse does with tidy evaluation.
Whenever you pipe a data frame to `mutate` or `filter` and so on, and you write expressions on unquoted variables, those arguments are (in a way) creating a new "partial" mutate function.
There is also delayed evaluation of that function: the unquoted expressions are not evaluated on the spot, but instead are translated to create the partial function you actually want.
It is that translated/partial function that actually is evaluated when you pass it your data frame.


